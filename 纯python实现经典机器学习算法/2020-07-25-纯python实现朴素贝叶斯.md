---
layout: post
title: "纯python实现经典机器学习算法之朴素贝叶斯算法"
date: 2020-07-26
description: "纯python实现经典机器学习算法"

tag: 纯python实现经典机器学习算法 
--- 

不调库，纯python实现经典的机器学习算法之朴素贝叶斯

代码放置在github上：[https://github.com/lxztju/machine_learning_python](https://github.com/lxztju/machine_learning_python)

理论知识请参考李航老师统计学习方法，

代码部分主要实现朴素贝叶斯，然后利用mnist数据集训练模型并测试准确率，采用logging模块输出日志信息。

主要的代码：

```python
    def naviebayes(self, x):
        '''
        利用朴素贝叶斯进行概率估计
        :param py: 先验概率
        :param pxy: 条件概率
        :param x: 待测样本点
        :return: 返回类别
        '''
        p= [0] * self.num_classes

        # 计算每个类别的概率
        for i in range(self.num_classes):
            # 由于在getProbaility中计算得到的概率值已经经过了log运算，因此这里的概率值可以采用连加的形式
            sum = 0
            for j in range(self.num_features):
                sum += self.px_y[i][j][x[j]]
            p[i] = sum + self.py[i]
        return p.index(max(p))


    def getProbability(self):
        '''
        计算所有训练集的先验与条件概率
        :param dataArr:  输入的训练样本集（list格式）
        :param labelArr:  输入的训练样本的label （list格式）
        :return:  返回训练集的先验概率分布与条件概率分布
        '''

        # 首先计算先验分布py，初始化py数组
        py = np.zeros((self.num_classes, 1))

        for i in range(self.num_classes):
            # 不考虑出现概率值为0的情况
            # np.mat(self.trainlabelArr == i)会让对应与等于i的为True， 不等的为False
            # py[i] = np.sum(np.mat(self.trainlabelArr == i)) / (len(self.trainlabelArr))

            # 考虑概率值为0的情况，采用laplace平滑
            py[i] = np.sum(np.mat(self.trainlabelArr == i) + 1) / (len(self.trainlabelArr) + self.num_classes)

        # 最后求后验概率估计的时候，形式是各项的相乘（“4.1 朴素贝叶斯法的学习” 式4.7），这里存在两个问题：1.某一项为0时，结果为0.
        # 这个问题通过分子和分母加上一个相应的数可以排除，前面已经做好了处理。2.如果特诊特别多（例如在这里，需要连乘的项目有784个特征
        # 加一个先验概率分布一共795项相乘，所有数都是0-1之间，结果一定是一个很小的接近0的数。）理论上可以通过结果的大小值判断， 但在
        # 程序运行中很可能会向下溢出无法比较，因为值太小了。所以人为把值进行log处理。log在定义域内是一个递增函数，也就是说log（x）中，
        # x越大，log也就越大，单调性和原数据保持一致。所以加上log对结果没有影响。此外连乘项通过log以后，可以变成各项累加，简化了计算。
        py = np.log(py)

        logging.info('Getting the prior distribution.')


        # 计算条件概率分布pxy，初始化pxy数组
        # 一共有num_classes类，一共有num_features个特征， 每个特征有两种取值，1或者0
        px_y = np.zeros((self.num_classes, self.num_features, 2))

        # 对标记集进行遍历
        for i in range(len(self.trainlabelArr)):
            # 获取当前循环所使用的标记
            label = self.trainlabelArr[i]
            # 获取当前要处理的样本
            x = self.traindataArr[i]
            # 对该样本的每一维特诊进行遍历
            for j in range(self.num_features):
                # 在矩阵中对应位置加1
                # 这里还没有计算条件概率，先把所有数累加，全加完以后，在后续步骤中再求对应的条件概率
                px_y[label][j][x[j]] += 1

        for label in range(self.num_classes):
            for j in range(self.num_features):
                # 分别计算第j个特征为0和1的个数
                px_y0 = px_y[label][j][0]
                px_y1 = px_y[label][j][1]

                # 计算条件概率
                px_y[label][j][0] = np.log((px_y0 +1) / (px_y0 + px_y1 + 2))
                px_y[label][j][1] = np.log((px_y1 +1) / (px_y0 + px_y1 + 2))
        logging.info('Getting the Conditional probability distribution.')

        return py, px_y
```



完整代码请移步github：[https://github.com/lxztju/machine_learning_python](https://github.com/lxztju/machine_learning_python)







其他算法的实现：

[纯python实现经典机器学习算法](https://zhuanlan.zhihu.com/p/163688301)





参考链接：https://github.com/Dod-o/Statistical-Learning-Method_Code
		https://github.com/fengdu78/lihang-code