---
layout: post
title: "纯python实现经典机器学习算法之决策树算法"
date: 2020-07-27
description: "纯python实现经典机器学习算法"

tag: 纯python实现经典机器学习算法 
--- 

不调库，纯python实现经典的机器学习算法之决策树

代码放置在github上：[https://github.com/lxztju/machine_learning_python](https://github.com/lxztju/machine_learning_python)

理论知识请参考李航老师统计学习方法，

代码部分主要实现ID3与C4.5的决策树，然后利用mnist数据集训练模型并测试准确率，采用logging模块输出日志信息。

## ID3决策树

算法主要部分为：经验熵，经验条件熵，信息增益，以及决策树的构建方法这几个主要的部分：

### 经验熵
```python
    def calculate_empirical_entropy(self, trainLabelArr):
        '''
        计算训练数据集的经验熵，公式参考李航老师统计学习方法
        :param trainLabelArr: numpy格式的label
        :return: 返回训练集的经验熵
        '''
        # 初始化经验熵为0
        H_D = 0
        # 这里为什么不采用self.num_classes直接调用，我刚开始也是这么写的
        # 后来发现如果在后期的计算中，某个类别不出现，那么log0会出现错误（参考README.md参考链接中大佬的利用set的实现）
        labels = set([label for label in trainLabelArr])
        for label in labels:

            # 根据公式需要计算每个类别的数目
            num = trainLabelArr[trainLabelArr==label].size
            # 计算每个类别占据数目占据整个数据集的比例
            p = num / trainLabelArr.size
            # 计算经验熵
            H_D += -1 *(p) * np.log2(p)

        return H_D
```


### 经验条件熵
```python
    def calculate_empirical_conditional_entropy(self, trainfeatureArr, trainlabelarr):
        '''
        计算经验条件熵
        :param trainfeatureArr: numpy格式的从数据集中抽离出某一个特征列
        :param trainlabelabelArr: numpy格式的label
        :return: 经验条件熵
        '''

        # 经验熵是对每个特征进行计算，因此应该返回一个列表，对于每个特征都进行计算分析
        # 桶计算经验熵时一样，采用set来选取特针的不同取值
        features = set([feature for feature in trainfeatureArr])
        H_D_A = 0
        for feature in features:
            # 计算取不同值时所包含的样本的数目
            Di = trainfeatureArr[trainfeatureArr == feature].size
            Di_D = Di / trainfeatureArr.size

            # 计算对于选取的特征取feature值时的条件熵

            H_D_A += Di_D * self.calculate_empirical_entropy(trainlabelarr[trainfeatureArr == feature])

        return H_D_A   
```



### 信息增益
```python
    def caculate_information_gain(self, traindataArr, trainlabelArr):
        '''
        :param traindataArr: 当前数据集的数组，numpy格式，因为每次在构建决策树机型分支的过程中，随着决策树层数的加深当前数据集会比越变越小
        :param trainlabelArr: 当前数据集的label数组，numpy格式
        计算最大的信息增益
        :return: 最大的信息增益及其对应的特征。
        '''
        # 获取当前数据集的特征数目
        num_features = traindataArr.shape[1]
        max_feature, max_G_D_A = 0, 0
        # 计算当前数据集的经验熵
        H_D = self.calculate_empirical_entropy(trainlabelArr)
        # 计算每个特征的经验条件熵
        for i in range(num_features):
            trainfeatureArr = traindataArr[:,i]
            H_D_i = self.calculate_empirical_conditional_entropy(trainfeatureArr, trainlabelArr)
            G_D_A = H_D - H_D_i
            if G_D_A > max_G_D_A:
                max_G_D_A = G_D_A
                max_feature  = i
        # 返回最大的信息增益，及其特征
        return max_feature, max_G_D_A   
```


### 决策树构建
```python
    def build_ID3tree(self, traindataArr, trainlabelArr):
        '''
        在数据集上递归构建决策树
        :param traindataArr: 当前节点为根节点对应的数据集 numpy
        :param trainlabelArr:  当前节点为根节点对应的数据集label numpy
        :return: 返回节点的值
        '''
        # 信息增益的阈值
        epsilon = 0.1


        # logging.info('Starting create a new Node. Now there are {} samples'.format(trainlabelArr.size))

        # 判断数据集此时的类别，如果只有一类，就范会对应的类别
        classDict = set(trainlabelArr)
        # print(classDict)
        if len(classDict) == 1:
            return int(classDict.pop())
        # print(traindataArr.shape)
        # 判断数据集此时的的特征数目，如果没有特征集，那就说明没有特征进行分割，就放会这些样本中数目最多的类别
        if len(traindataArr.shape) == 1:
            return self.majorClass(trainlabelArr)
        # 计算最大增益及其对应的特征
        Ag, G_D_Ag = self.caculate_information_gain(traindataArr, trainlabelArr)
        # print(Ag, G_D_Ag)
        # 如果最大的信息增益小于设定的阈值，就直接返回数目最多的类，不必要进行分割
        if G_D_Ag < epsilon:
            return self.majorClass(trainlabelArr)

        tree = {Ag:{}}
        # 递归构建决策树
        features = set(feature for feature in traindataArr[:, Ag])
        for feature in features:
            a = int(feature)
            newdataArr, newlabelArr = self.updateDataSet(traindataArr, trainlabelArr, Ag, a)

            tree[Ag][a] = self.build_ID3tree(newdataArr, newlabelArr)
        # print(tree)
        return tree 
```

## C4.5决策树

C4.5决策树与ID3决策树不同的地方就是这里才用信息增益比来作为特征分割的标志

### 信息增益比的计算

```python
       def calculate_HDA(self, traindataArr, A):
        '''
        计算数据集关于特征的A
        :param traindataArr: 训练数据集， numpy格式
        :param A:  特征A
        :return: 返回Ha(D)熵值
        '''
        HDA = 0
        features = set([feature for feature in traindataArr[:,A]])
        for feature in features:
            if traindataArr[:, A][traindataArr[:, A]== feature].size == 0:
                print(traindataArr, traindataArr.shape, features)
            p = traindataArr[:, A][traindataArr[:, A]== feature].size / traindataArr[:, A].size
            if p == 1:
                HDA = 1
            else:
                HDA += -1 * p * np.log2(p)

    
    
    def calculate_information_gain_ratio(self, traindataArr, trainlabelArr):
        '''
        :param traindataArr: 当前数据集的数组，numpy格式，因为每次在构建决策树机型分支的过程中，随着决策树层数的加深当前数据集会比越变越小
        :param trainlabelArr: 当前数据集的label数组，numpy格式
        计算最大的信息增益
        :return: 最大的信息增益及其对应的特征。
        '''
        # 获取当前数据集的特征数目
        num_features = traindataArr.shape[1]
        max_feature, max_gain = 0, 0
        # 计算当前数据集的经验熵
        H_D = self.calculate_empirical_entropy(trainlabelArr)
        # 计算每个特征的经验条件熵
        for i in range(num_features):
            trainfeatureArr = traindataArr[:,i]
            H_D_i = self.calculate_empirical_conditional_entropy(trainfeatureArr, trainlabelArr)
            G_D_A = H_D - H_D_i
            H_A_D = self.calculate_HDA(traindataArr, i)
            # if H_A_D == 0: return
            gain = G_D_A / H_A_D
            if gain > max_gain:
                max_gain = gain
                max_feature  = i
        # 返回最大的信息增益，及其特征
        return max_feature, max_gain
```



完整代码请移步github：[https://github.com/lxztju/machine_learning_python](https://github.com/lxztju/machine_learning_python)







其他算法的实现：

[纯python实现经典机器学习算法](https://zhuanlan.zhihu.com/p/163688301)





参考链接：https://github.com/Dod-o/Statistical-Learning-Method_Code
		https://github.com/fengdu78/lihang-code